{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefkong1982/netology.ru/blob/Master/Kompyuternoe_zrenie/Zadachi_detekcii_segmentacii/DZ_Kondratev_The_Nature_Conservancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание**\n",
        "\n",
        "**Преподаватель:** Наталья Баданина, Юлия Пономарева, Ярослав Сапронов, Артём Качалкин, Павел Мехнин\n",
        "\n",
        "В данном задании вам предстоит помочь [The Nature Conservancy](https://www.nature.org/en-us/about-us/where-we-work/united-states/california/) в борьбе с рыбаками-браконьерами. Необходимо по снимкам с камеры на рыболовецком судне классифицировать улов. За основу решения предлагается взять модель детектора:\n",
        "[github.com...tion.ipynb](https://github.com/a4tunado/lectures/blob/master/007/007-detection.ipynb). Решение необходимо прислать в виде ссылки на ipython-ноутбук с указанием значения метрики на [Leaderboard](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/leaderboard)."
      ],
      "metadata": {
        "id": "Dz7esUMQgOI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Импорт библиотек и загрузка данных"
      ],
      "metadata": {
        "id": "GBiOjbMmizGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFDadDti04FL"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import zipfile\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "5xQunNTI1N9J",
        "outputId": "19936f3e-148e-4a22-cb5e-7ddda33d6b1d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b2594eb3-601c-4e5d-93f6-36c762fbe654\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b2594eb3-601c-4e5d-93f6-36c762fbe654\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"stefkon82\",\"key\":\"b01f82295f6caca3fd66207958de205d\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# подключим kaggle и скачаем датасет\n",
        "from google.colab import files\n",
        "# нужно выбрать kaggle.json\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv7dzivK1Q85",
        "outputId": "4851346f-97fd-46b3-927a-f58f119aa13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the-nature-conservancy-fisheries-monitoring.zip to /content\n",
            "100% 2.10G/2.11G [00:30<00:00, 18.8MB/s]\n",
            "100% 2.11G/2.11G [00:31<00:00, 72.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# скачиваем датасет\n",
        "!kaggle competitions download -c the-nature-conservancy-fisheries-monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6nUXVtv9lN8"
      },
      "outputs": [],
      "source": [
        "!apt-get install p7zip-full -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkXbcKbs4vZv"
      },
      "outputs": [],
      "source": [
        "!unzip /content/the-nature-conservancy-fisheries-monitoring.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/sample_submission_stg1.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZATRhqEcEVR",
        "outputId": "4f98bef6-0a20-40e2-a438-5dac01073019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/sample_submission_stg1.csv.zip\n",
            "  inflating: sample_submission_stg1.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/sample_submission_stg2.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpfJ9Q-ZcMca",
        "outputId": "204aeb96-81e2-429d-9af5-6c0bdec684fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/sample_submission_stg2.csv.zip\n",
            "  inflating: sample_submission_stg2.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Чтение содержимого sample_submission_stg1.csv\n",
        "df1 = pd.read_csv('/content/sample_submission_stg1.csv')\n",
        "print(\"Первые строки из sample_submission_stg1.csv:\")\n",
        "print(df1.head())\n",
        "\n",
        "# Чтение содержимого sample_submission_stg2.csv\n",
        "df2 = pd.read_csv('/content/sample_submission_stg2.csv')\n",
        "print(\"\\nПервые строки из sample_submission_stg2.csv:\")\n",
        "print(df2.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOOwUr9bEHEs",
        "outputId": "f0e5b4d1-1c1b-464b-a0e4-91be7707b515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Первые строки из sample_submission_stg1.csv:\n",
            "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
            "0  img_00005.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "1  img_00007.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "2  img_00009.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "3  img_00018.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "4  img_00027.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "\n",
            "      SHARK       YFT  \n",
            "0  0.046585  0.194283  \n",
            "1  0.046585  0.194283  \n",
            "2  0.046585  0.194283  \n",
            "3  0.046585  0.194283  \n",
            "4  0.046585  0.194283  \n",
            "\n",
            "Первые строки из sample_submission_stg2.csv:\n",
            "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
            "0  img_00005.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "1  img_00007.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "2  img_00009.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "3  img_00018.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "4  img_00027.jpg  0.455003  0.052938  0.030969  0.017734  0.123081  0.079142   \n",
            "\n",
            "      SHARK       YFT  \n",
            "0  0.046585  0.194283  \n",
            "1  0.046585  0.194283  \n",
            "2  0.046585  0.194283  \n",
            "3  0.046585  0.194283  \n",
            "4  0.046585  0.194283  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/train.zip"
      ],
      "metadata": {
        "id": "1BLQOVKcfvhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/test_stg1.zip"
      ],
      "metadata": {
        "id": "XJMJGo0zaZay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq9ayOVc9Hse"
      },
      "outputs": [],
      "source": [
        "!7z x test_stg2.7z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLrv29AE8Jt3"
      },
      "outputs": [],
      "source": [
        "!unzip /content/boxes.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpmBjjTq_JP4"
      },
      "outputs": [],
      "source": [
        "!apt-get install tree -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hhCZJ1L_GUz",
        "outputId": "23987f57-ec66-4781-be3c-88ab3dafd353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m.\u001b[0m\n",
            "├── \u001b[01;34mboxes\u001b[0m\n",
            "├── \u001b[01;31mboxes.zip\u001b[0m\n",
            "├── \u001b[00mkaggle.json\u001b[0m\n",
            "├── \u001b[01;34m__MACOSX\u001b[0m\n",
            "├── \u001b[00mmy_model.h5\u001b[0m\n",
            "├── \u001b[01;34msample_data\u001b[0m\n",
            "├── \u001b[01;31msample_submission_stg1.csv.zip\u001b[0m\n",
            "├── \u001b[01;31msample_submission_stg2.csv.zip\u001b[0m\n",
            "├── \u001b[01;34mtest_stg1\u001b[0m\n",
            "├── \u001b[01;31mtest_stg1.zip\u001b[0m\n",
            "├── \u001b[01;34mtest_stg2\u001b[0m\n",
            "├── \u001b[00mtest_stg2.7z\u001b[0m\n",
            "├── \u001b[01;31mthe-nature-conservancy-fisheries-monitoring.zip\u001b[0m\n",
            "├── \u001b[01;34mtrain\u001b[0m\n",
            "└── \u001b[01;31mtrain.zip\u001b[0m\n",
            "\n",
            "6 directories, 9 files\n"
          ]
        }
      ],
      "source": [
        "!tree -L 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTgmO7EU_pzV",
        "outputId": "50f3bc65-3ef8-4e1f-acc7-addff992cb21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alb_labels.json  dol_labels.json  shark_labels.json\n",
            "bet_labels.json  lag_labels.json  yft_labels.json\n"
          ]
        }
      ],
      "source": [
        "%ls /content/boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEQvJusVAy_R",
        "outputId": "2db30b78-7a47-4458-e644-51c19fb8ea62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mALB\u001b[0m/  \u001b[01;34mBET\u001b[0m/  \u001b[01;34mDOL\u001b[0m/  \u001b[01;34mLAG\u001b[0m/  \u001b[01;34mNoF\u001b[0m/  \u001b[01;34mOTHER\u001b[0m/  \u001b[01;34mSHARK\u001b[0m/  \u001b[01;34mYFT\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls /content/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K963a8gBS_S",
        "outputId": "583fd587-480a-4046-f951-94f78b629f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_00001.jpg\n",
            "image_00002.jpg\n",
            "image_00003.jpg\n",
            "image_00004.jpg\n",
            "image_00005.jpg\n",
            "image_00006.jpg\n",
            "image_00007.jpg\n",
            "image_00008.jpg\n",
            "image_00009.jpg\n",
            "image_00010.jpg\n"
          ]
        }
      ],
      "source": [
        "%ls /content/test_stg2 | head -n 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Подготовка данных"
      ],
      "metadata": {
        "id": "qu6qmpzmjzx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Определим путь к директории с аннотациями\n",
        "boxes_dir = '/content/boxes'\n",
        "\n",
        "# Список классов рыб с аннотациями bounding box\n",
        "fish_classes = ['alb', 'bet', 'dol', 'lag', 'shark', 'yft']\n",
        "\n",
        "annotations = {}\n",
        "\n",
        "# Загружаем аннотации для каждого класса\n",
        "for fish in fish_classes:\n",
        "    json_file = os.path.join(boxes_dir, f'{fish}_labels.json')\n",
        "    with open(json_file, 'r') as f:\n",
        "        annotations[fish] = json.load(f)"
      ],
      "metadata": {
        "id": "mlL8dGuf0Fl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qKu92_QGFQ3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Определяем параметры аугментации данных\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,           # Масштабируем пиксельные значения\n",
        "    rotation_range=30,        # Случайные повороты\n",
        "    width_shift_range=0.1,    # Случайные горизонтальные сдвиги\n",
        "    height_shift_range=0.1,   # Случайные вертикальные сдвиги\n",
        "    shear_range=0.2,          # Сдвиги\n",
        "    zoom_range=0.2,           # Зум\n",
        "    horizontal_flip=True,     # Случайные горизонтальные отражения\n",
        "    fill_mode='nearest',      # Режим заполнения для трансформированных пикселей\n",
        "    validation_split=0.2      # 20% данных для валидации\n",
        ")\n",
        "\n",
        "# Для валидационных данных только масштабирование\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "yLH896X80M8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwdBEMwfGUMT",
        "outputId": "6dc49f61-36ab-47b0-f3ed-89e821d808ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3025 images belonging to 8 classes.\n",
            "Found 752 images belonging to 8 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "image_size = (224, 224)  # MobileNetV2 ожидает изображения размером 224x224\n",
        "\n",
        "# Определение директории с обучающими данными\n",
        "train_dir = '/content/train'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',       # Устанавливаем как обучающую выборку\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',     # Устанавливаем как валидационную выборку\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Создание и компиляция модели"
      ],
      "metadata": {
        "id": "L4ap92rKj6wm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g71yfkFQH0j7",
        "outputId": "4bbd8748-3645-4eaf-9d3e-f2a470dc3b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Загружаем базовую модель\n",
        "base_model = MobileNetV2(\n",
        "    weights='imagenet',        # Загружаем веса, предварительно обученные на ImageNet\n",
        "    include_top=False,         # Не включаем классификатор\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "# Замораживаем слои базовой модели\n",
        "base_model.trainable = False\n",
        "\n",
        "# Добавляем собственные верхние слои\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "outputs = Dense(8, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Обучение модели"
      ],
      "metadata": {
        "id": "_N414YOjj-jl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI7ofiZXH3Ip",
        "outputId": "91760a7b-2839-4b75-cb50-061768a5c57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 2s/step - accuracy: 0.5004 - loss: 1.6104 - val_accuracy: 0.6037 - val_loss: 1.0605\n",
            "Epoch 2/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 2s/step - accuracy: 0.6412 - loss: 0.9571 - val_accuracy: 0.6582 - val_loss: 0.9518\n",
            "Epoch 3/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 2s/step - accuracy: 0.7044 - loss: 0.8126 - val_accuracy: 0.7128 - val_loss: 0.7911\n",
            "Epoch 4/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 2s/step - accuracy: 0.7591 - loss: 0.7056 - val_accuracy: 0.7327 - val_loss: 0.7450\n",
            "Epoch 5/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 2s/step - accuracy: 0.7668 - loss: 0.6515 - val_accuracy: 0.7846 - val_loss: 0.6144\n",
            "Epoch 6/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 2s/step - accuracy: 0.8160 - loss: 0.5377 - val_accuracy: 0.8045 - val_loss: 0.6060\n",
            "Epoch 7/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 2s/step - accuracy: 0.8085 - loss: 0.5410 - val_accuracy: 0.7926 - val_loss: 0.6205\n",
            "Epoch 8/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 2s/step - accuracy: 0.8314 - loss: 0.4889 - val_accuracy: 0.7846 - val_loss: 0.6151\n",
            "Epoch 9/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - accuracy: 0.8497 - loss: 0.4333 - val_accuracy: 0.7620 - val_loss: 0.6505\n",
            "Epoch 10/10\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 2s/step - accuracy: 0.8501 - loss: 0.4319 - val_accuracy: 0.7979 - val_loss: 0.5884\n"
          ]
        }
      ],
      "source": [
        "# Обучение модели\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Промежуточные результаты обучения"
      ],
      "metadata": {
        "id": "B-W4qI2JkJiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы наблюдаем рост точности на валидационном наборе данных, что указывает на успешное обучение модели.\n"
      ],
      "metadata": {
        "id": "dD1My3T2kKat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение всей модели в формате .h5\n",
        "model.save('my_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weoK4nKt3bGC",
        "outputId": "68f6e63f-51f7-4b2e-e4fd-20be4a4a556a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Скачивание сохраненной модели\n",
        "files.download('my_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KXADvDfX4CVU",
        "outputId": "75816dc0-b073-47e8-b375-8ba1bb8804bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_69dbb4ab-7830-4916-8cfb-8d06088e6e90\", \"my_model.h5\", 13362632)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "uFUUwLSUg4iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Загрузка заранее обученной модели\n",
        "model = load_model('my_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL_PY3t1a-Mx",
        "outputId": "7a88a64a-3330-4795-ceee-a295bf8593b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Предсказание на тестовых данных"
      ],
      "metadata": {
        "id": "AWITHHvqkXK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Пути к директории с тестовыми изображениями stg1\n",
        "test_dir1 = '/content/test_stg1'\n",
        "\n",
        "# Загрузка файла с образцом для первого этапа\n",
        "sample_submission1 = pd.read_csv('sample_submission_stg1.csv')\n",
        "\n",
        "# Получаем список идентификаторов изображений из образца\n",
        "image_ids_stage1 = sample_submission1['image'].tolist()\n",
        "\n",
        "# Проверка количества изображений\n",
        "print(f\"Количество изображений для Stage 1: {len(image_ids_stage1)}\")\n",
        "\n",
        "# Подготовка к предсказанию\n",
        "image_size = (224, 224)\n",
        "predictions1 = []\n",
        "\n",
        "# Проходим по всем тестовым изображениям в порядке, указанном в sample_submission\n",
        "for image_id in image_ids_stage1:\n",
        "    # Получаем имя файла без пути\n",
        "    image_file = os.path.basename(image_id)\n",
        "    img_path = os.path.join(test_dir1, image_file)\n",
        "\n",
        "    # Проверяем, существует ли файл\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Изображение {img_path} не найдено.\")\n",
        "        continue\n",
        "\n",
        "    # Загружаем и обрабатываем изображение\n",
        "    img = load_img(img_path, target_size=image_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0  # Нормализация\n",
        "\n",
        "    # Предсказание\n",
        "    pred = model.predict(img_array, verbose=0)\n",
        "    predictions1.append(pred[0])\n",
        "\n",
        "# Создаем DataFrame для отправки\n",
        "submission_df1 = pd.DataFrame(predictions1, columns=sample_submission1.columns[1:])  # Пропускаем первую колонку 'image'\n",
        "submission_df1.insert(0, 'image', sample_submission1['image'])  # Используем колонку 'image' из эталонного файла\n",
        "\n",
        "# Сохранение DataFrame в CSV\n",
        "submission_df1.to_csv('submission_stg1.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc8QaIivIFsr",
        "outputId": "c4fffa78-5c4b-4874-e5cc-588c86050849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество изображений для Stage 1: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Путь к директории с тестовыми изображениями Stage 2\n",
        "test_dir2 = '/content/test_stg2'\n",
        "\n",
        "# Получаем список всех файлов в директории\n",
        "image_files = os.listdir(test_dir2)\n",
        "\n",
        "# Фильтруем только файлы с расширением .jpg (если это необходимо)\n",
        "image_files = [f for f in image_files if f.lower().endswith('.jpg')]\n",
        "\n",
        "# Проверка количества изображений\n",
        "print(f\"Количество изображений для Stage 2: {len(image_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-C2-pbdI06v",
        "outputId": "7a1369a2-6ce2-4fcc-8d9f-6e477ea5d9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество изображений для Stage 2: 12153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка к предсказанию\n",
        "predictions2 = []\n",
        "image_ids = []\n",
        "\n",
        "# Размер изображений, который использовался при обучении модели (замените на ваш размер)\n",
        "image_size = (224, 224)  # Это пример, замените на ваши значения\n",
        "\n",
        "# Проходим по всем тестовым изображениям\n",
        "for image_file in image_files:\n",
        "    img_path = os.path.join(test_dir2, image_file)\n",
        "\n",
        "    # Проверяем, что файл существует (хотя мы уже получили список существующих файлов, это на всякий случай)\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Изображение {img_path} не найдено.\")\n",
        "        continue\n",
        "\n",
        "    # Загружаем и обрабатываем изображение\n",
        "    img = load_img(img_path, target_size=image_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0  # Нормализация\n",
        "\n",
        "    # Предсказание\n",
        "    pred = model.predict(img_array, verbose=0)\n",
        "    predictions2.append(pred[0])  # Предполагается, что модель возвращает массив предсказаний\n",
        "    image_ids.append(image_file)"
      ],
      "metadata": {
        "id": "KrsI7a1cI6wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Предполагается, что у вас есть список названий классов\n",
        "class_names = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']  # Замените на ваши классы, если они отличаются\n",
        "\n",
        "# Создаем DataFrame из предсказаний\n",
        "submission_df2 = pd.DataFrame(predictions2, columns=class_names)\n",
        "submission_df2.insert(0, 'image', image_ids)  # Добавляем столбец с именами изображений\n",
        "\n",
        "# Добавляем префикс 'test_stg2/' к именам файлов в столбце 'image'\n",
        "submission_df2['image'] = 'test_stg2/' + submission_df2['image']\n",
        "\n",
        "# Смотрим на первые строки DataFrame\n",
        "print(submission_df2.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seZwTBZLR9Wc",
        "outputId": "8f606fff-686c-41c1-9fd9-aa67e363f896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       image       ALB       BET       DOL           LAG  \\\n",
            "0  test_stg2/image_02630.jpg  0.060685  0.008218  0.000321  2.263525e-04   \n",
            "1  test_stg2/image_10681.jpg  0.279469  0.000718  0.000073  1.283372e-08   \n",
            "2  test_stg2/image_01756.jpg  0.078455  0.000264  0.000220  5.055735e-08   \n",
            "3  test_stg2/image_03779.jpg  0.693738  0.075645  0.080729  1.578407e-04   \n",
            "4  test_stg2/image_06384.jpg  0.154048  0.000663  0.000370  1.749551e-04   \n",
            "\n",
            "        NoF     OTHER     SHARK       YFT  \n",
            "0  0.168046  0.003221  0.396883  0.362399  \n",
            "1  0.679361  0.000223  0.000048  0.040107  \n",
            "2  0.915437  0.000162  0.000025  0.005437  \n",
            "3  0.003258  0.126641  0.001953  0.017879  \n",
            "4  0.103730  0.036144  0.050371  0.654501  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение DataFrame в CSV\n",
        "submission_df2.to_csv('submission_stg2.csv', index=False)"
      ],
      "metadata": {
        "id": "VcZ_H-FnSeq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка файлов предсказаний\n",
        "submission_stg1 = pd.read_csv('submission_stg1.csv')\n",
        "submission_stg2 = pd.read_csv('submission_stg2.csv')\n",
        "\n",
        "# Объединяем два датафрейма\n",
        "combined_submission = pd.concat([submission_stg1, submission_stg2], ignore_index=True)\n",
        "\n",
        "# Проверяем размер нового файла\n",
        "print(f\"Combined Submission Shape: {combined_submission.shape}\")\n",
        "\n",
        "# Сохранение объединенного файла в CSV\n",
        "combined_submission.to_csv('combined_submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7qtyk_JStVA",
        "outputId": "a5c2ee07-275d-4e6e-bdd5-c547c8e6843f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Submission Shape: (13153, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Финальная отправка результатов"
      ],
      "metadata": {
        "id": "NF9gVhCOkb-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c the-nature-conservancy-fisheries-monitoring -f combined_submission.csv -m \"Combined submission for stage 1 and stage 2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qe1Zz_RI9mU",
        "outputId": "3541b856-b9df-4ccc-cc4a-ad95edfce016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 1.55M/1.55M [00:00<00:00, 7.71MB/s]\n",
            "Successfully submitted to The Nature Conservancy Fisheries Monitoring"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Получение результатов в переменную\n",
        "result = subprocess.run([\"kaggle\", \"competitions\", \"submissions\", \"-c\", \"the-nature-conservancy-fisheries-monitoring\"], capture_output=True, text=True)\n",
        "\n",
        "# Разделение на строки и вывод первых\n",
        "lines = result.stdout.splitlines()\n",
        "print(\"\\n\".join(lines[:3]))  # Печатает первые строки"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoipmDr9yRmd",
        "outputId": "d9838d35-2745-4396-9bdf-c87f5714acea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fileName                          date                 description                                  status    publicScore  privateScore  \n",
            "--------------------------------  -------------------  -------------------------------------------  --------  -----------  ------------  \n",
            "combined_submission.csv           2024-11-26 13:47:42  Combined submission for stage 1 and stage 2  complete  1.64284      3.29151       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Результаты и дальнейшие шаги\n",
        "\n",
        "Мы успешно завершили этапы обучения, валидации и предсказания для двух тестовых наборов. Предложим следующее:\n",
        "\n",
        "1. Провести большее количество эпох для обучения, чтобы повысить точность модели.\n",
        "2. Использовать более сложные стратегии аугментации, чтобы улучшить обобщающую способность модели.\n",
        "3. Рассмотреть возможность использования методов регуляризации для предотвращения переобучения.\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Созданная классификационная модель позволяет автоматизировать процесс идентификации улова и способствует борьбе с браконьерством подводных ресурсов. Благодаря работе с различными методами обработки и аугментации данных, а также дообучению предварительно обученной модели, удалось достичь удовлетворительных результатов в классификации видов рыб."
      ],
      "metadata": {
        "id": "i1YJVU7yk5Ff"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}