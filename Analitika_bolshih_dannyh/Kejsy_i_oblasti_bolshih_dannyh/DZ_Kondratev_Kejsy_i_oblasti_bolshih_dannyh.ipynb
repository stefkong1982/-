{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefkong1982/netology.ru/blob/Master/Analitika_bolshih_dannyh/Kejsy_i_oblasti_bolshih_dannyh/DZ_Kondratev_Kejsy_i_oblasti_bolshih_dannyh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Самостоятельное домашнее задание к занятию «Кейсы и области применения больших данных»\n",
        "\n",
        "**Преподаватель:** Алексей Кузьмин\n",
        "\n",
        "**Цель задания**\n",
        "\n",
        "Научиться решать практические задачи с использованием pandas или pyspark на основе реальных данных и провести работу с данными от загрузки данных до построения модели.\n",
        "\n",
        "**Важно**\n",
        "\n",
        "Это задание не будет проверено экспертом, однако, стоит его выполнить, чтобы отработать теорию и с лёгкостью справляться с подобными задачами на практике.\n",
        "\n",
        "**Контекст**\n",
        "\n",
        "Дан набор данных по оттоку клиентов. Набор данных содержит всего 5 000 записей (то есть абонентов).\n",
        "\n",
        "Данные доступны [тут](https://drive.google.com/open?id=1ArslqEEno2hrr5tAs25P0JN0P-coLcFD)\n",
        "\n",
        "Перечень полей:\n",
        "\n",
        "* state – штат\n",
        "* account length – абонентский стаж\n",
        "* area code – код региона\n",
        "* phone number – номер телефона\n",
        "* international plan – тарифный план для международных звонков\n",
        "* voice mail plan – тарифный план для голосовой почты\n",
        "* number vmail messages – количество сообщений голосовой почты\n",
        "* total day minutes – общая длительность звонков в дневное время (мин)\n",
        "* total day calls – общее количество звонков в дневное время\n",
        "* total day charge – общая стоимость звонков в дневное время\n",
        "* total eve minutes – общая длительность звонков в вечернее время (мин)\n",
        "* total eve calls – общее количество звонков в вечернее время\n",
        "* total eve charge – общая стоимость звонков в вечернее время\n",
        "* total night minutes – общая длительность звонков в ночное время (мин)\n",
        "* total night calls – общее количество звонков в ночное время\n",
        "* total night charge – общая стоимость звонков в ночное время\n",
        "* total intl minutes – общая длительность международных звонков (мин)\n",
        "* total intl calls – общее количество международных звонков\n",
        "* total intl charge – общая стоимость международных звонков\n",
        "* number customer service calls – количество звонков в службу поддержки\n",
        "* churned – покинул ли клиент компанию\n",
        "\n",
        "**Описание задания**\n",
        "\n",
        "Ваша задача - используя `panda`s или `pyspark`, ответить на следующие вопросы:\n",
        "\n",
        "\n",
        "\n",
        "1. Построить гистограмму количества звонков в техническую поддержку\n",
        "2. Рассчитать и построить гистограмму общей длительности звонков клиента (дневных + ночных + вечерних + международных)\n",
        "3. Собственноручно (не используя встроенных функций) рассчитать линейный коэффициент корреляции (доп. материалы общего количества минут и количества звонков в техподдержку\n",
        "4. Визуализировать точечный график по общему количеству минут / количеству звонков в поддержку, подкрасив точки в зависимости от оттока абонента\n",
        "5. Вывести top-5 самых много и самых мало говорящих клиентов\n",
        "6. Вывести долю оттока клиентов и среднюю стоимость минуты дневного времени разговора в зависимости от штата\n",
        "7. Перевести штат в one-hot формат при помощи pandas-функции get_dummies или удалить колонку, если вы делаете решение на pyspark\n",
        "8. Разбить данные на множества для обучения и для проверки, отобрав признаки для обучения модели классификации (убрать номер телефона, код региона, признаки планов + все добавленные атрибуты)\n",
        "9. Обучить какую-нибудь модель классификации и оценить качество (точность) на отложенной выборке\n",
        "\n",
        "**Формат выполнения**\n",
        "\n",
        "Выполните задание в Google Colaboratory (Python) с помощью pandas или pyspark.\n",
        "\n",
        "**Результат выполненного задания**\n",
        "\n",
        "* Для корректной сдачи домашнего задания необходимо в личном кабинете прикреплять ссылку на ваше решение в Google Colab.\n",
        "* После того, как вы прикрепите ссылку на своё решение в личном кабинете, вам станет доступно эталонное решение, которое подготовил эксперт. Проверьте правильность выполнения этого задания самостоятельно, сравнив свой документ с эталоном.\n",
        "* Если в процессе выполнения самостоятельного домашнего задания у вас возникнут вопросы, их можно будет обсудить с экспертом на разборном вебинаре."
      ],
      "metadata": {
        "id": "ZXHGm7bul18a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkjqeMDxpo-a"
      },
      "source": [
        "Установка Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etFJ7ZgKpl41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42dfbd17-edf9-477a-f34b-42ace4e3bd2d"
      },
      "source": [
        "# Обновляем пакеты системы с помощью apt-get update\n",
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 3,626 B/3,626 B 100%] [Connected to ppa.\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.8\r                                                                                                    \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                                                    \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,735 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,339 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,822 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,784 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,455 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,065 kB]\n",
            "Fetched 9,541 kB in 3s (3,464 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0HaFRkEps1B"
      },
      "source": [
        "# Устанавливаем OpenJDK 8 с помощью apt-get install.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy_sIJ6ypyIv"
      },
      "source": [
        "# Скачиваем архив Spark с официального сайта с помощью wget.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q5LMILepz_N"
      },
      "source": [
        "# Распаковываем архив Spark с помощью tar -xvf.\n",
        "!tar -xvf spark-3.5.0-bin-hadoop3.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiQOFODip19a"
      },
      "source": [
        "# Устанавливаем findspark с помощью pip install.\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJCFdpyJp4Oe"
      },
      "source": [
        "# Устанавливаем переменные окружения JAVA_HOME и SPARK_HOME для работы с Spark.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализируем findspark для работы с Spark.\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Создаем экземпляр SparkSession для работы с Spark.\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Получаем контекст SparkContext для работы с Spark.\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "iRGY9sdmFNp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка данных"
      ],
      "metadata": {
        "id": "7YHTh16z4k8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Загрузите данные\n",
        "df = pd.read_csv('churn.csv')\n",
        "\n",
        "# Постройте гистограмму количества звонков в техническую поддержку\n",
        "df['number customer service calls'].value_counts().plot(kind='bar')\n"
      ],
      "metadata": {
        "id": "Etnqphj_DWBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "1ed107cc-d843-4170-ca94-49506b1114aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'churn.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9d9f6cbd0143>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Загрузите данные\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'churn.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Постройте гистограмму количества звонков в техническую поддержку\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'churn.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отображаем первые 5 строк датафрейма.\n",
        "df.show(150)"
      ],
      "metadata": {
        "id": "sj0Mbw4G090m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### При помощи VectorAssembler преобразовать все колонки с признаками в одну (использовать Pipeline — опционально).\n"
      ],
      "metadata": {
        "id": "klvlesUK40lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# В данном коде мы импортируем модули Vectors и VectorAssembler из PySpark.\n",
        "# Модуль Vectors содержит классы и функции для работы с векторами признаков,\n",
        "# а модуль VectorAssembler используется для сборки признаков в вектор."
      ],
      "metadata": {
        "id": "RY-XVXr74_YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Метод df.columns возвращает список столбцов датафрейма df.\n",
        "\n",
        "df.columns"
      ],
      "metadata": {
        "id": "whLkOZiPy4HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование признаков:\n",
        "# Функция VectorAssembler используется для преобразования признаков\n",
        "# 'sepal_length', 'sepal_width', 'petal_length', 'petal_width' в вектор признаков 'Features'.\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\n",
        " 'sepal_length',\n",
        " 'sepal_width',\n",
        " 'petal_length',\n",
        " 'petal_width',\n",
        " ], outputCol='Features')"
      ],
      "metadata": {
        "id": "RFxDtGc50DCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = assembler.transform(df)\n",
        "\n",
        "# В данном коде мы применяем функцию VectorAssembler к датафрейму df с помощью метода transform().\n",
        "# Метод transform() применяет функцию к каждому ряду датафрейма и возвращает новый датафрейм с преобразованными данными.\n",
        "# В результате применения VectorAssembler к df, мы получаем новый датафрейм, в котором все признаки\n",
        "# 'sepal_length', 'sepal_width', 'petal_length', 'petal_width' объединены в один столбец 'Features'."
      ],
      "metadata": {
        "id": "nQ2BJJ0p1tGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "id": "T0KnF8rP0j21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Разобьем данные на данные для обучения и проверки"
      ],
      "metadata": {
        "id": "SJU8qQVaWv7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# Разбиение данных на обучающую и тестовую выборки:\n",
        "# Метод randomSplit() используется для случайного разделения датафрейма\n",
        "# на обучающую и тестовую выборки с заданными пропорциями.\n",
        "\n",
        "#В данном случае, 80% данных используется для обучения, а 20% - для тестирования.\n",
        "\n",
        "# Параметр seed=12345 в методе randomSplit()\n",
        "# используется для задания начального значения генератора случайных чисел.\n",
        "\n",
        "# Это позволяет получить воспроизводимые результаты разделения данных на обучающую и тестовую выборки.\n",
        "# Если вы используете один и тот же код и зададите тот же seed, то получите одни и те же случайные числа,\n",
        "# и, следовательно, одни и те же обучающую и тестовую выборки"
      ],
      "metadata": {
        "id": "NO4HzbICT8kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.show(5)"
      ],
      "metadata": {
        "id": "6UiPxGbLpQmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создадим и обучим модель логистической регрессии"
      ],
      "metadata": {
        "id": "lnJHcaSA5XPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# В данном коде мы импортируем класс LogisticRegression из модуля pyspark.ml.classification.\n",
        "# Класс LogisticRegression используется для обучения модели логистической регрессии."
      ],
      "metadata": {
        "id": "s0d9_QT8WlSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(featuresCol = 'Features', labelCol = 'variety_num')\n",
        "lrModel = lr.fit(train)\n",
        "\n",
        "# Обучение модели логистической регрессии:\n",
        "# Создается экземпляр класса LogisticRegression\n",
        "# с указанием столбца признаков 'Features' и столбца меток 'variety_num'.\n",
        "\n",
        "# Затем модель обучается на обучающей выборке с помощью метода fit()."
      ],
      "metadata": {
        "id": "uvrsUBz92UzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_res = lrModel.transform(train)\n",
        "test_res = lrModel.transform(test)\n",
        "\n",
        "# В данном коде мы применяем обученную модель логистической регрессии lrModel\n",
        "# к обучающей выборке train и тестовой выборке test с помощью метода transform().\n",
        "\n",
        "# Метод transform() применяет модель к данным и возвращает новый датафрейм с предсказаниями модели.\n",
        "# В результате, мы получаем датафреймы train_res и test_res, которые содержат исходные данные и предсказания модели для каждой строки."
      ],
      "metadata": {
        "id": "lzpggDOp2rZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pandas = train_res.toPandas()\n",
        "df_pandas.head(1)\n",
        "\n",
        "# Данный код преобразует датафрейм train_res из PySpark в pandas.DataFrame с помощью метода toPandas().\n",
        "# Затем он выводит на экран первую строку преобразованного датафрейма с помощью метода head(1).\n",
        "# Этот код позволяет просмотреть первые несколько строк датафрейма train_res как полноценный датафрейм в pandas."
      ],
      "metadata": {
        "id": "77YLuvws2wbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Значения `rawPrediction` представляют собой скалярное произведение вектора признаков и вектора коэффициентов модели.\n",
        "\n",
        "* `67.70279195875312` - это предсказание модели для класса `Setosa`.\n",
        "\n",
        "* `-5.483728693221446` - это предсказание модели для класса `Versicolor`.\n",
        "\n",
        "* `-62.21906326553166` - это предсказание модели для класса `Virginica`.\n",
        "\n",
        "Чем больше это значение, тем больше вероятность того, что наблюдение принадлежит к соответствующему классу.\n",
        "\n",
        "\n",
        "2. Pначения `probability` представляют собой вероятности принадлежности каждого наблюдения к каждому из классов.\n",
        "\n",
        "* `1.0` - это вероятность принадлежности наблюдения к классу `Setosa`.\n",
        "\n",
        "* `1.642471835404166e-32` - это вероятность принадлежности наблюдения к классу `Versicolor`.\n",
        "\n",
        "\n",
        "* `3.764048385223648e-57` - это вероятность принадлежности наблюдения к классу `Virginica`.\n",
        "\n",
        "Это значение может быть интерпретировано как степень уверенности модели в принадлежности данного наблюдения к классу. Чем ближе значение к 1, тем больше вероятность того, что наблюдение принадлежит к соответствующему классу.\n",
        "\n",
        "3. `prediction` - это предсказанный класс для каждого наблюдения. В данном случае, наблюдение было предсказано как класс `Setosa`.\n",
        "\n"
      ],
      "metadata": {
        "id": "AIwcNxT3Jvpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Оценим качество\n",
        "\n",
        "Для оценки качества предсказания в spark реализованно несколько классов\n",
        "\n",
        "Если мы решаем задачу бинарной классификации (то есть классов - 2),\n",
        "\n",
        "то нам подойдет `BinaryCLassificationEvaluator`,\n",
        "\n",
        "а если классов больше 2-х, то `MulticlassClassificationEvaluator`"
      ],
      "metadata": {
        "id": "-ttDlzuedZCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Библиотека pyspark.ml.evaluation содержит классы и функции\n",
        "# для оценки качества моделей машинного обучения в PySpark.\n",
        "\n",
        "# В данном случае, мы используем класс MulticlassClassificationEvaluator\n",
        "# для оценки качества модели логистической регрессии на основе метрики точности."
      ],
      "metadata": {
        "id": "_WJzd21g2v31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ev = MulticlassClassificationEvaluator(labelCol='variety_num')\n",
        "\n",
        "# Оценка качества модели: Для оценки качества модели используется MulticlassClassificationEvaluator.\n",
        "# Создается экземпляр класса MulticlassClassificationEvaluator с указанием столбца меток 'variety_num'."
      ],
      "metadata": {
        "id": "TX3UVe-zgqRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Метод evaluate() применяется к результатам обучения и тестирования модели. Значение точности выводится на экран.\n",
        "\n",
        "ev.evaluate(train_res)\n",
        "print(\"Точность модели на обучающей выборке: %.2f\" % ev.evaluate(train_res))\n",
        "\n",
        "ev.evaluate(test_res)\n",
        "print(\"Точность модели на тестовой выборке: %.2f\" % ev.evaluate(test_res))"
      ],
      "metadata": {
        "id": "tgtP-OIsYIT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучите модель дерева решений и оцените его качество\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oP6zkL66d2R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "S0QEv8VucaHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr = DecisionTreeClassifier(featuresCol='Features', labelCol='variety_num')\n",
        "trFitted = tr.fit(train)\n",
        "\n",
        "# Обучение модели дерева решений:\n",
        "# Аналогично предыдущему шагу, создается экземпляр класса DecisionTreeClassifier\n",
        "# с указанием столбца признаков 'Features' и столбца меток 'variety_num'.\n",
        "# Затем модель обучается на обучающей выборке с помощью метода fit()."
      ],
      "metadata": {
        "id": "0t4jhIVj4oXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tr_res=trFitted.transform(train)\n",
        "test_tr_res=trFitted.transform(test)\n",
        "\n",
        "# В данном коде мы применяем обученную модель логистической регрессии lrModel\n",
        "# к обучающей выборке train и тестовой выборке test с помощью метода transform()."
      ],
      "metadata": {
        "id": "3Ir9Epj76btx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tr_res.show(5)"
      ],
      "metadata": {
        "id": "4JSs8Dsk6b6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Метод evaluate() применяется к результатам обучения и тестирования модели. Значение точности выводится на экран.\n",
        "\n",
        "ev.evaluate(train_tr_res)\n",
        "print(\"Точность модели дерева решений на обучающей выборке: %.2f\" % ev.evaluate(train_tr_res))\n",
        "\n",
        "ev.evaluate(test_tr_res)\n",
        "print(\"Точность модели дерева решений на тестовой выборке: %.2f\" % ev.evaluate(test_tr_res))"
      ],
      "metadata": {
        "id": "qpEN4bGg6hy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По результатам оценки моделей логистической регрессии и дерева решений с использованием библиотеки PySpark можно сделать следующие выводы:\n",
        "\n",
        "1. Модель логистической регрессии показала высокую точность на обучающей выборке (98%) и отличную точность на тестовой выборке (100%). Это говорит о том, что модель хорошо обобщает данные и способна делать точные прогнозы как на известных, так и на новых данных.\n",
        "\n",
        "2. Модель дерева решений также продемонстрировала высокую точность на обучающей выборке (99%) и отличную точность на тестовой выборке (100%). Это указывает на то, что модель дерева решений эффективно обучается на данных и хорошо обобщает информацию для делания точных прогнозов.\n",
        "\n",
        "Обе модели показали высокие показатели точности как на обучающей, так и на тестовой выборках, что свидетельствует о их хорошей способности к классификации. Однако, у модели логистической регрессии наблюдается небольшое снижение точности на тестовой выборке по сравнению с обучающей, в то время как у модели дерева решений точность остается на одном уровне. Таким образом, обе модели могут быть использованы для классификации данных, но стоит учитывать особенности каждой из них при принятии окончательного решения."
      ],
      "metadata": {
        "id": "GppiO8K1gVWj"
      }
    }
  ]
}