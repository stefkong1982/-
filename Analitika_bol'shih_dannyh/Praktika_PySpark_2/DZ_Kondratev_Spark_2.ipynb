{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefkong1982/netology.ru/blob/Master/Analitika_bol'shih_dannyh/Praktika_PySpark_2/DZ_Kondratev_Spark_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Домашнее задание «Практика PySpark (часть 1)»\n",
        "\n",
        "**Преподаватель:** Алексей Кузьмин\n",
        "\n",
        "Домашнее задание:\n",
        "\n",
        "* [скачайте dataset iris](https://drive.google.com/file/d/18ksAxTxBkp15LToEg46BHhwp3sPIoeUU/view?usp=sharing)\n",
        "\n",
        "* [решите домашнее задание](https://colab.research.google.com/drive/1mbJisCLaZJ4QuamBJl2tEVhRTWkSFdxy?usp=sharing)"
      ],
      "metadata": {
        "id": "ZXHGm7bul18a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkjqeMDxpo-a"
      },
      "source": [
        "Установка Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etFJ7ZgKpl41",
        "outputId": "0fd18780-f690-478b-c7e6-a7bc2a44673c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Обновляем пакеты системы с помощью apt-get update\n",
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [1 InRelease 0 B/3,626 \r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [Connecting to ppa.laun\r                                                                                                    \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [671 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,436 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,746 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,334 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,800 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,723 kB]\n",
            "Fetched 8,944 kB in 2s (4,234 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0HaFRkEps1B"
      },
      "source": [
        "# Устанавливаем OpenJDK 8 с помощью apt-get install.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy_sIJ6ypyIv"
      },
      "source": [
        "# Скачиваем архив Spark с официального сайта с помощью wget.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q5LMILepz_N"
      },
      "source": [
        "# Распаковываем архив Spark с помощью tar -xvf.\n",
        "!tar -xvf spark-3.4.2-bin-hadoop3.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiQOFODip19a"
      },
      "source": [
        "# Устанавливаем findspark с помощью pip install.\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJCFdpyJp4Oe"
      },
      "source": [
        "# Устанавливаем переменные окружения JAVA_HOME и SPARK_HOME для работы с Spark.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализируем findspark для работы с Spark.\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Создаем экземпляр SparkSession для работы с Spark.\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Получаем контекст SparkContext для работы с Spark.\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "iRGY9sdmFNp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузите данные при помощи spark.read.csv из приложенного файла"
      ],
      "metadata": {
        "id": "7YHTh16z4k8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Читаем данные из файла 'iris.CSV' в датафрейм Spark.\n",
        "df = spark.read.csv('iris.CSV', inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "fepAuogPQhST",
        "outputId": "d24f66af-9dca-4b23-e752-96fb8040c60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/iris.CSV.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bb35c27d90b3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Читаем данные из файла 'iris.CSV' в датафрейм Spark.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris.CSV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.4.2-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.2-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/iris.CSV."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отображаем первые 5 строк датафрейма.\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "sj0Mbw4G090m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Воспользуйтесь командами groupBy, max, min и avg, чтобы вывести максимальное минимальное и среднее значние каждого из аттрбутов цветка\n",
        "\n",
        "(`sepal_length`,`sepal_width`,`petal_length`,`petal_width`)"
      ],
      "metadata": {
        "id": "klvlesUK40lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем новый датафрейм df_grp, группируя исходный датафрейм df по столбцу 'variety'.\n",
        "df_grp = df.groupBy('variety')"
      ],
      "metadata": {
        "id": "nOqLsHZq2UqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем новый датафрейм df_max, вычисляя максимальные значения для столбцов для каждой группы в df_grp.\n",
        "df_max = df_grp.max('sepal_length','sepal_width','petal_length','petal_width')\n",
        "df_max.show()"
      ],
      "metadata": {
        "id": "lMGduUHn2laG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем новый датафрейм df_min, вычисляя минимальные значения для столбцов для каждой группы в df_grp.\n",
        "df_min = df_grp.min('sepal_length','sepal_width','petal_length','petal_width')\n",
        "df_min.show()"
      ],
      "metadata": {
        "id": "zkYFcXTW3EbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем новый датафрейм df_avg, вычисляя средние значения для столбцов для каждой группы в df_grp.\n",
        "df_avg = df_grp.avg('sepal_length','sepal_width','petal_length','petal_width')\n",
        "df_avg.show()"
      ],
      "metadata": {
        "id": "kodFkxb23ntb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RY-XVXr74_YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### График Зависимости длины и ширины чашелистиков"
      ],
      "metadata": {
        "id": "SJU8qQVaWv7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Отображение точек для каждого класса с уникальными цветами\n",
        "df_pd = df.toPandas()\n",
        "plt.scatter(df_pd[df_pd['variety_num']==0]['sepal_length'], df_pd[df_pd['variety_num']==0]['sepal_width'], label='Setosa', c='r')\n",
        "plt.scatter(df_pd[df_pd['variety_num']==1]['sepal_length'], df_pd[df_pd['variety_num']==1]['sepal_width'], label='Versicolor', c='g')\n",
        "plt.scatter(df_pd[df_pd['variety_num']==2]['sepal_length'], df_pd[df_pd['variety_num']==2]['sepal_width'], label='Virginica', c='b')\n",
        "\n",
        "plt.xlabel('Длина чашелистика (см)')\n",
        "plt.ylabel('Ширина чашелистика (см)')\n",
        "plt.title('Зависимость длины и ширины чашелистиков')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NO4HzbICT8kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd.head(5)"
      ],
      "metadata": {
        "id": "6UiPxGbLpQmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выведите по аналогии график petal_length и petal_width, так же раскрашенный по типам цветков"
      ],
      "metadata": {
        "id": "lnJHcaSA5XPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Зависимость длины и ширины лепестков\n",
        "plt.scatter(df_pd[df_pd['variety_num']==0]['petal_length'], df_pd[df_pd['variety_num']==0]['petal_width'], label='Setosa', c='r')\n",
        "plt.scatter(df_pd[df_pd['variety_num']==1]['petal_length'], df_pd[df_pd['variety_num']==1]['petal_width'], label='Versicolor', c='g')\n",
        "plt.scatter(df_pd[df_pd['variety_num']==2]['petal_length'], df_pd[df_pd['variety_num']==2]['petal_width'], label='Virginica', c='b')\n",
        "\n",
        "plt.xlabel('Длина лепестка (см)')\n",
        "plt.ylabel('Ширина лепестка (см)')\n",
        "plt.title('Зависимость длины и ширины лепестков')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s0d9_QT8WlSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Посмотрите внимательно на получившийся график и сформируйте правило\n",
        "вида:\n",
        "\n",
        " ЕСЛИ ЗНАЧЕНИЕ ПАРАМЕТРА `A` < `ЗНАЧЕНИЕ`, ТО `IRIS` ОТНОСИТСЯ К КЛАССУ `SETOSA` - ИНАЧЕ К КАКОМУ-ТО ДРУГОМУ (НАЗОВЕМ, ЕГО НАПРИМЕР `UNKNOWN`)\n",
        "\n",
        "\n",
        "При помощи команд `withColumn`, `when`, `otherwise` создайте в датафрейме новую колонку с таким предсказанием"
      ],
      "metadata": {
        "id": "-ttDlzuedZCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "# Библиотека pyspark.sql.functions предоставляет функции для работы с данными в PySpark.\n",
        "# Она содержит множество полезных функций, таких как when, col, sum, mean и другие,\n",
        "# которые позволяют манипулировать данными, выполнять агрегацию, фильтрацию и многое другое."
      ],
      "metadata": {
        "id": "TX3UVe-zgqRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Если значение столбца \"petal_length\" меньше 2, то в новой колонке \"prediction\" будет записано \"Setosa\".\n",
        "# В противном случае, будет записано \"Unknown\".\n",
        "\n",
        "df = df.withColumn(\"prediction\", when(col(\"petal_length\") < 2, \"Setosa\").otherwise(\"Unknown\"))\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "tgtP-OIsYIT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Проверьте качество своего предсказания\n",
        "Для этого:\n",
        "\n",
        "1.   Подсчитайте количество строк, где Вы предсказали класс Setosa и правильный класс был так же Setosa\n",
        "2.   Подсчитайте количество строк, где Вы предскзали класс Setosa, а правильный класс был какой-то другой (воспользуйтесь оператором \"не равно\" - !=)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oP6zkL66d2R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Подсчитываем количество строк, где предсказали класс Setosa и правильный класс был так же Setosa.\n",
        "correct_setosa = df.where(df['prediction'] == 'Setosa').count()\n",
        "\n",
        "# Подсчитываем количество строк, где предсказали класс Setosa, а правильный класс был какой-то другой.\n",
        "wrong_setosa = df.where((df['prediction'] == 'Setosa') & (df['variety'] != 'Setosa')).count()\n",
        "\n",
        "print(f'Количество строк, где предсказали класс Setosa и правильный класс был так же Setosa: {correct_setosa}')\n",
        "print(f'Количество строк, где предсказали класс Setosa, а правильный класс был какой-то другой: {wrong_setosa}')"
      ],
      "metadata": {
        "id": "S0QEv8VucaHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}