{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefkong1982/netology.ru/blob/Master/%D0%91%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B8%20Python%20%D0%B4%D0%BB%D1%8F%20%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F%20%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D0%B2%D0%B5%D0%B1-%D1%81%D0%BA%D1%80%D0%B0%D0%BF%D0%B8%D0%BD%D0%B3%D0%B0/%D0%94%D0%97_%D0%9A%D0%BE%D0%BD%D0%B4%D1%80%D0%B0%D1%82%D1%8C%D0%B5%D0%B2_NUMPY-48%20%D0%BA%20%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B8%20%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D0%B2%D0%B5%D0%B1-%D1%81%D0%BA%D1%80%D0%B0%D0%BF%D0%B8%D0%BD%D0%B3%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ovTcpjbdUi"
      },
      "source": [
        "# Домашнее задание к лекции \"Основы веб-скрапинга\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3p6bH0WbdUo"
      },
      "source": [
        "## Обязательная часть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZz0rUMbdUp"
      },
      "source": [
        "Вам необходимо написать функцию, которая будет основана на **поиске** по сайту [habr.com](https://habr.com/ru/search/).\n",
        "Функция в качестве параметра должна принимать **список** запросов для поиска (например, `['python', 'анализ данных']`) и на основе материалов, попавших в результаты поиска по **каждому** запросу, возвращать датафрейм вида:\n",
        "\n",
        "```\n",
        "<дата> - <заголовок> - <ссылка на материал>\n",
        "```\n",
        "\n",
        "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xa70bBfMbdUq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import urllib.parse\n",
        "\n",
        "base_url = 'https://habr.com'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get('https://habr.com/ru/search/?q=анализ%20данных&target_type=posts&order=relevance')\n",
        "res"
      ],
      "metadata": {
        "id": "T1-H3ADhf7Jl",
        "outputId": "aa187fec-e183-4829-d199-5f3b560e2983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(res.text)\n",
        "soup"
      ],
      "metadata": {
        "id": "RPnNinREgXXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = soup.find_all('article', class_='tm-articles-list__item')\n",
        "print(len(news))\n",
        "print(news[0])"
      ],
      "metadata": {
        "id": "MIc35QX3gmh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import urllib.parse\n",
        "\n",
        "base_url = 'https://habr.com'\n",
        "res = requests.get('https://habr.com/ru/search/?q=анализ%20данных&target_type=posts&order=relevance')\n",
        "soup = BeautifulSoup(res.text)\n",
        "news = soup.find_all('article', class_='tm-articles-list__item')\n",
        "\n",
        "for article in news:\n",
        "    # Проверка наличия заголовка в оригинальной разметке\n",
        "    title_element = article.find('h2', class_='tm-megapost-snippet__title')\n",
        "    if title_element is not None:\n",
        "        title = title_element.text.strip()\n",
        "        print(\"Заголовок:\", title)\n",
        "    else:\n",
        "        # Проверка наличия заголовка в другой разметке\n",
        "        title_element_alt = article.find('h2', class_='tm-title tm-title_h2')\n",
        "        if title_element_alt is not None:\n",
        "            title = title_element_alt.find('span').text.strip()\n",
        "            print(\"Заголовок:\", title)\n",
        "        else:\n",
        "            print(\"Заголовок не найден\")\n",
        "\n",
        "    # Проверка наличия ссылки в оригинальной разметке\n",
        "    link_element = article.find('a', class_='tm-megapost-snippet__link tm-megapost-snippet__date')\n",
        "    if link_element is not None:\n",
        "        relative_link = link_element.get('href')\n",
        "        full_link = urllib.parse.urljoin(base_url, relative_link)\n",
        "        print(full_link)\n",
        "    else:\n",
        "        # Проверка наличия ссылки в другой разметке\n",
        "        link_element_alt = article.find('h2', class_='tm-title tm-title_h2').find('a', class_='tm-title__link')\n",
        "        if link_element_alt is not None:\n",
        "            relative_link = link_element_alt.get('href')\n",
        "            full_link = urllib.parse.urljoin(base_url, relative_link)\n",
        "            print(full_link)\n",
        "        else:\n",
        "            print(\"Ссылка не найдена\")\n",
        "\n",
        "    # Проверка наличия даты в оригинальной разметке\n",
        "    date_element = article.find('time', class_='tm-megapost-snippet__datetime-published')\n",
        "    if date_element is not None:\n",
        "        date = date_element.get('title')\n",
        "        print(\"Дата:\", date)\n",
        "    else:\n",
        "        # Проверка наличия даты в другой разметке\n",
        "        date_element_alt = article.find('span', class_='tm-article-datetime-published').find('time')\n",
        "        if date_element_alt is not None:\n",
        "            date = date_element_alt.get('title')\n",
        "            print(\"Дата:\", date)\n",
        "        else:\n",
        "            print(\"Дата не найдена\")\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "VnS1OpDZtQ-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "import time\n",
        "\n",
        "def get_habr_posts(query):\n",
        "    \"\"\"\n",
        "    Получает заголовки и ссылки на статьи с сайта habr.com по заданному запросу.\n",
        "\n",
        "    Аргументы:\n",
        "    query -- список запросов для поиска статей (например, ['python', 'анализ данных'])\n",
        "\n",
        "    Возвращает:\n",
        "    DataFrame -- датафрейм с информацией о заголовках и ссылках на статьи\n",
        "    \"\"\"\n",
        "\n",
        "    habr_blog = pd.DataFrame()\n",
        "    base_url = \"https://habr.com\"\n",
        "\n",
        "    for q in query:\n",
        "        URL = 'https://habr.com/ru/search/'\n",
        "        params = {\n",
        "            'q': q\n",
        "        }\n",
        "        req = requests.get(URL, params=params)\n",
        "        time.sleep(0.3)\n",
        "        soup = BeautifulSoup(req.text)\n",
        "        articles = soup.find_all('article', class_='tm-articles-list__item')\n",
        "\n",
        "        if not articles:\n",
        "            continue\n",
        "\n",
        "        posts = []\n",
        "        for article in articles:\n",
        "            title_element = article.find('h2', class_='tm-megapost-snippet__title')\n",
        "            if title_element is not None:\n",
        "                title = title_element.text.strip()\n",
        "                link_element = article.find('a', class_='tm-megapost-snippet__link tm-megapost-snippet__date')\n",
        "                relative_link = link_element.get('href')\n",
        "                full_link = urllib.parse.urljoin(base_url, relative_link)\n",
        "                date_element = article.find('time', class_='tm-megapost-snippet__datetime-published')\n",
        "                date = date_element.get('title')\n",
        "            else:\n",
        "                title_element_alt = article.find('h2', class_='tm-title tm-title_h2')\n",
        "                title = title_element_alt.find('span').text.strip()\n",
        "                link_element_alt = article.find('h2', class_='tm-title tm-title_h2').find('a', class_='tm-title__link')\n",
        "                relative_link = link_element_alt.get('href')\n",
        "                full_link = urllib.parse.urljoin(base_url, relative_link)\n",
        "                date_element_alt = article.find('span', class_='tm-article-datetime-published').find('time')\n",
        "                date = date_element_alt.get('title')\n",
        "\n",
        "            post = {'date': date, 'title': title, 'link': full_link}\n",
        "            posts.append(post)\n",
        "\n",
        "\n",
        "        habr_blog = pd.concat([habr_blog, pd.DataFrame(posts)], ignore_index=True)\n",
        "\n",
        "    # Удаление дубликатов\n",
        "    habr_blog = habr_blog.drop_duplicates()\n",
        "\n",
        "    return habr_blog\n",
        "\n",
        "# Пример использования функции\n",
        "res = get_habr_posts(['python', 'анализ данных'])\n",
        "res"
      ],
      "metadata": {
        "id": "TgIi0OrATrKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYsz6he4bdUt"
      },
      "source": [
        "## Дополнительная часть (необязательная)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFzgIo3rbdUu"
      },
      "source": [
        "Функция из обязательной части задания должна быть расширена следующим образом:\n",
        "- кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента `4` необходимо получить материалы с первых 4 страниц результатов;\n",
        "- в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
        "```\n",
        "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}